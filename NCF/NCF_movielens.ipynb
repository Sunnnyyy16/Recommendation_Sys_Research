{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp37-cp37m-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl (266.3 MB)\n",
      "     -------------------------------------- 266.3/266.3 MB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.10.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 KB 3.8 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-win_amd64.whl (896 kB)\n",
      "     -------------------------------------- 896.6/896.6 KB 4.7 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 KB 3.0 MB/s eta 0:00:00\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.3 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 5.5 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     ------------------------------------ 439.2/439.2 KB 670.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     ---------------------------------------- 24.4/24.4 MB 4.3 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.10.0.2)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 KB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (58.0.4)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.0-cp37-cp37m-win_amd64.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 KB 3.8 MB/s eta 0:00:00\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.9/93.9 KB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.8/181.8 KB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.4)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.3/181.3 KB 3.6 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.9/83.9 KB 2.4 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 KB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.56.0 keras-2.11.0 libclang-16.0.0 markdown-3.4.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\ProgramData\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25424/2609088937.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mseed_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "def seed_everything(seed:int=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "!wget https://files.grouplens.org/datasets/movielens/ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "!unzip -o ml-100k.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_column_names =  ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "user_df = pd.read_csv(\"ml-100k/u.user\", sep='|', names=user_column_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_column_names = ['movie_id', 'movie_title' ,'release_date','video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure',\n",
    " 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    " 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "item_df = pd.read_csv('ml-100k/u.item', sep='|', names=item_column_names, encoding='latin-1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구조\n",
    "reference : \n",
    "\n",
    "https://github.com/LeeHyeJin91/Neural_CF\n",
    "\n",
    "https://github.com/hexiangnan/neural_collaborative_filtering\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    # 사용할 layer 작성\n",
    "    def __init__(self,user_num,item_num, layers):\n",
    "        \n",
    "        # user embedding\n",
    "        user = Input(shape=(1,))\n",
    "        # user_embedding = Embedding(user_num, 32, input_length=user.shape[1])(user)\n",
    "        user_embedding = Embedding(user_num, 32, input_length=user.shape[1])(user)\n",
    "        user_embedding = Flatten()(user_embedding)\n",
    "        \n",
    "        # item embedding\n",
    "        item = Input(shape=(1,))\n",
    "        # item_embedding = Embedding(item_num, 32, input_length = item.shape[1])(item)\n",
    "        item_embedding = Embedding(item_num, 32, input_length = item.shape[1])(item)\n",
    "        item_embedding = Flatten(item_embedding)\n",
    "        \n",
    "        # Merge\n",
    "        # concat = concatenate()([user_embedding, item_embedding])\n",
    "        concatenated = concatenate([user_embedding, item_embedding])\n",
    "        dropout = Dropout(0.2)(concatenated)\n",
    "        \n",
    "        # layer 1\n",
    "        layer1= Dense(64, activation='relu', name='layer1')(dropout)\n",
    "        dropout1 = Dropout(0.2, name='dropout1')(layer1)\n",
    "        batch_norm1 = BatchNormalization(name='batch_norm1')(dropout1)\n",
    "        \n",
    "        # layer 2\n",
    "        layer2 = Dense(32, activation='relu',name='layer2')(batch_norm1)\n",
    "        dropout2 = Dropout(0.2, name='dropout2')(layer2)\n",
    "        batch_norm2 = BatchNormalization(name='batch_norm2')(dropout2)\n",
    "        \n",
    "        # layer 3\n",
    "        layer3 = Dense(16, activation='relu', name='layer3')(batch_norm2)\n",
    "        \n",
    "        # layer 4\n",
    "        layer4 = Dense(8, activation='relu', name='layer4')(layer3)\n",
    "        \n",
    "        # output\n",
    "        output_layer = Dense(1, kernel_initializer='lecun_uniform', name='output_layer')(layer4)\n",
    "        #lecun_uniform : [-limit, limit] 범위내 균등분포에 따라 샘플 생성\n",
    "        \n",
    "        # Model\n",
    "        self.model = Model([user,item], output_layer)\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        \n",
    "        def get_model(self):\n",
    "            model = self.model\n",
    "            return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GMF():\n",
    "    def __init__(self, user_num, item_num):\n",
    "        \n",
    "        LATENT_SIZE = 8\n",
    "        \n",
    "        # user embedding\n",
    "        user = Input((1,))\n",
    "        user_embedding = Embedding(user_num, LATENT_SIZE, input_length = user.shape[1])(user)\n",
    "        user_embedding = Flatten(user_embedding)\n",
    "        \n",
    "        # item embedding\n",
    "        item = Input((1,))\n",
    "        item_embedding = Embedding(item_num, LATENT_SIZE, input_length = item.shape[1])(item)\n",
    "        item_embedding = Flatten(item_embedding)\n",
    "        \n",
    "        # Merge\n",
    "        multi = Multiply([user_embedding, item_embedding])\n",
    "        \n",
    "        # output\n",
    "        output_layer = Dense(1, activation='sigmoid', kernel_initializer = 'lecun_uniform', name = 'output_layer')(multi)\n",
    "\n",
    "        # Model\n",
    "        self.model = Model([user,item], output_layer)\n",
    "        self.model.compile(optimizer='adam', loss= 'binary_crossentropy')\n",
    "        \n",
    "    def get_model(self):\n",
    "        model = self.model\n",
    "        return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF():\n",
    "    def __init__(self, user_num, item_num):\n",
    "        LATENT_SIZE=8\n",
    "        \n",
    "        # input\n",
    "        user = Input(1,)\n",
    "        item = Input(1,)\n",
    "        \n",
    "        # MLP user embedding\n",
    "        mlp_user_embedding = Embedding(user_num, 32, input_length = user.shape[1])(user)\n",
    "        mlp_user_embedding = Flatten(mlp_user_embedding)\n",
    "        \n",
    "        # MLP item embedding\n",
    "        mlp_item_embedding = Embedding(item_num, 32, input_length = item.shape[1])(item)\n",
    "        mlp_item_embedding = Flatten(mlp_item_embedding)\n",
    "        \n",
    "        # GMF user embedding\n",
    "        gmf_user_embedding = Embedding(user_num, LATENT_SIZE, input_length = user.shape[1])(user)\n",
    "        gmf_user_embedding = Flatten(gmf_user_embedding)\n",
    "        \n",
    "        # GMF item embedding\n",
    "        gmf_item_embedding = Embedding(item_num, LATENT_SIZE, input_length = item.shape[1])(item)\n",
    "        gmf_item_embedding = Flatten(gmf_item_embedding)\n",
    "        \n",
    "        # MLP Merge\n",
    "        mlp_concatenated = concatenate([mlp_user_embedding, mlp_item_embedding])\n",
    "        mlp_dropout = Dropout(0.2)(mlp_concatenated)\n",
    "        \n",
    "        # GMF Merge\n",
    "        gmf_multiply = Multiply([gmf_user_embedding, gmf_item_embedding])\n",
    "        \n",
    "        # MLP layer1\n",
    "        mlp_layer1 = Dense(64, activation='relu', name='mlp_layer1')(mlp_dropout)\n",
    "        mlp_dropout1 = Dropout(0.2)(mlp_layer1)\n",
    "        mlp_batch_norm1 = BatchNormalization()(mlp_dropout1)\n",
    "        \n",
    "        # MLP layer2\n",
    "        mlp_layer2 = Dense(32, activation='relu', name= 'mlp_layer2')(mlp_batch_norm1)\n",
    "        mlp_dropout2 = Dropout(0.2)(mlp_layer2)\n",
    "        mlp_batch_norm2 = BatchNormalization()(mlp_dropout2)\n",
    "        \n",
    "        # MLP layer3\n",
    "        mlp_layer3 = Dense(16, activation='relu', name='mlp_layer3')(mlp_batch_norm2)\n",
    "        \n",
    "        # MLP layer4\n",
    "        mlp_layer4 = Dense(8, activation='relu', name='mlp_layer4')(mlp_layer3)\n",
    "        \n",
    "        # MLP + GMF\n",
    "        final_concatenated = concatenate([mlp_layer4, gmf_multiply])\n",
    "        \n",
    "        # output layer\n",
    "        output_layer = Dense(1, activation='sigmoid', kernel_initializer = 'lecun_uniform', name=output_layer)(final_concatenated)\n",
    "        \n",
    "        # model\n",
    "        self.model = Model([user,item], output_layer)\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        \n",
    "    def get_model(self):\n",
    "        model = self.model\n",
    "        return model\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "class Metrix:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_hits(self, k_ranked, holdout):\n",
    "        for item in k_ranked:\n",
    "            if item == holdout:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    def eval_rating(self, idx, test_ratings, test_negatives, K, model):\n",
    "        # holdout(df_Test의 item)이 k순위 내에 있는지 평가하는 함수\n",
    "        items = test_negatives[idx]\n",
    "        user_idx = test_ratings[idx][0]\n",
    "        holdout = test_ratings[idx][1]\n",
    "        items.append(holdout)\n",
    "        \n",
    "        # prediction\n",
    "        predict_user = np.full(len(items), user_idx, dtype='int32').reshape(-1,1)\n",
    "        np_items = np.array(items).reshape(-1,1)\n",
    "        \n",
    "        predictions = model.predict([predict_user, np_items])\n",
    "        predictions = predictions.flatten().tolist()\n",
    "        item_to_pre_score = {item:pre for item, pre in zip(items, predictions)}\n",
    "        \n",
    "        # 점수가 높은 상위 k개 아이템 리스트\n",
    "        k_ranked = heapq.nlargest(K,item_to_pre_score, key=item_to_pre_score.get)\n",
    "        \n",
    "        # holdout이 상위 K순위에 포함되는지\n",
    "        # {1: 포함, 0: 포함x}\n",
    "        hits = self.get_hits(k_ranked, holdout)\n",
    "        \n",
    "        return hits\n",
    "    \n",
    "    def evaluate_top_k(self, df_neg, df_test, model, K=10):\n",
    "        # Top- K metric을 사용해 모델 평가하는 함수\n",
    "        hits = []\n",
    "        test_u = df_test['user_id'].values.tolist()\n",
    "        test_i = df_test['item_id'].values.tolist()\n",
    "        \n",
    "        test_ratings = list(zip(test_u, test_i))\n",
    "        df_neg = df_neg.drop(df_neg.columns[0], axis=1)\n",
    "        test_negatives = df_neg.values.tolist()\n",
    "        \n",
    "        # user 샘플링\n",
    "        sample_idx_lst = np.random.choice(len(test_ratings), int(len(test_ratings)))\n",
    "        for user_idx in sample_idx_lst:\n",
    "            \n",
    "            hitrate = self.eval_rating(user_idx, test_ratings, test_negatives, K, model)\n",
    "            hits.append(hitrate)\n",
    "            \n",
    "        return hits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
